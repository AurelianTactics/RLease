# working 31923
rethink approach and initial goals to get something working an useful
think what big picture things would be

# working 122722

get basic functionality of release branch going for eval single agent
	make a run_single_agent.py
		args for evaluate and for trajectory
		just do the evaluate for now, for trajectory do a warning
DONE review non-integrated and misc files and organize them so know what is integrated and not
make examples
make docs
	installation
	basic usage


#working 102822

potential next things
	think bigger picture, running many agents, and saving results
	check other ideas for evaluations
	fill out some of the to dos
	custom evals based on info
		thinking build this up as you experience them


to test
TEST basic eval framework
	NOT NEEDED add an argument to run_trajectory_main.py for reward and longer
	DONE collect it in trajectory_stats
	DONE add more plotting in trajectory plot
	DONE longer term things

minimal eval code
	avg reward
	avg episode length
	basic plots
	to do
		histograms, max, min reward, other ideas from blogs and packages

get minimal action and vf dist code and example up
	simple ray cartpole
		DONE create conda env for ray21
		DONE install ray21
		DONE install matching torch/tf
		DONE create simple checkpoint
		load checkpoint and smoke test it
	TEST outerframework
	TEST create agent
	TEST create env
	evaluation code: distribution actions, traj vf. see email
TEST MA sp
TEST probe envs
	fill out the env
		keys
		args
		logic
	test in notebook
	test it can train
TEST corr between end goal reward
	some scrap code in rlease_vf_function_plots.ipynb for this
		test it
		move into python script
		print plots maybe?
		track key stats in aggregate
		option for end goal
mid goal reward
	write code in rlease_vf_function_plots.ipynb
		do non zero option and zero option
		for non zero, subset df by != zero and take corr of two columns
		for the other steps shift the df where possible
			might have to drop early reward if not something x steps behind it 

Longer term
	basic plots
		random seeds, confidence intervals, etc from that paper
		min, max, histograms
		other ideas

		
# working old
* DONE think how you would abstract it for parallel, lot of loops, lot of evaluations 
	# DONE how you would describe the process (did stratego have a picture?)
# moving code from analysis to mazs class
	DONE fill in adding agents to existing dict
	DONE finish up creating MAZS dict 
	DONE go to next step in run() and move it over 
	fill out the play game function

# in play_game of mzsa.py
	# fill out the play game results to figure out the dicts
	# tidy up the dicts needed, maybe just do one mega dict with keys
	# one of the dicts uses game_number as a key, how to handle that

# version 0.001
* DONE move the basic botbowl vf code into repo
* DONE move the basic botbowl trueskill and eval code into the repo
* DONE integrate the trueskill code with some of the multiagent_zero_sum.py code
* DONE tidy up/stub out the multiagent_zero_sum_analysis.py code
* TEST integrate true_skill with the multi agent code
	* too much plotting and printing in the true skill, abstract this
* pritn function for WLD and H2H need to be abstracted
* save function probably needs to be extracted
	* at least add a to do section: more dynamic saving, cloud saving, result by result saving

* Tidy up, see notes below
* get basic agent eval loop working with botbowl
* make/stubout some basic pandas plot functions
* make list of to do and improvements
	* save results as they occur, batch meta results instead of all at once


# version 0.002
* get the vf/reward plotting code to be abstracted a bit
	* trajectory grabber and preparer code
	* ipynb code
* get it to work with some env or library of sorts

rlease improvement overview
	I want a class and not functions since different envs and eval types will have different things attached to the classes even if usign the same function
		ie attach an ELO class or a TRUESKILL class etc
	for now doing the 5 dicts, but put them all into 1 since it is easier to pass round
		and assign to self
	then run time, instantiate the class with args and a set up
		then either in the loop or in the class use the args to run it and get results
	then when results dict is saved, use print and pandas to display some results

# longer term
	loading and abstracting the agent input could be done better maybe? maybe think it through for best user experience
	every env/problem usually has some KPI that are custom. way to make this code flexible enough for that
	turn into a package that can be downloaded and installed
