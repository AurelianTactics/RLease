{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2439f57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-21 19:57:27</td></tr>\n",
       "<tr><td>Running for: </td><td>00:11:16.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.7/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/2.42 GiB heap, 0.0/1.21 GiB objects (0.0/1.0 accelerator_type:GTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_0f402_00000</td><td>TERMINATED</td><td>192.168.0.18:10319</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         667.143</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">     164</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 124</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:14,024\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:14,025\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10362)\u001b[0m /home/jim/anaconda3/envs/rlease2022/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10362)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m /home/jim/anaconda3/envs/rlease2022/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m 2022-11-21 19:46:17,613\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:17,699\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname   </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                              </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                      </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                 </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_0f402_00000</td><td style=\"text-align: right;\">                 400000</td><td>{&#x27;num_env_steps_sampled&#x27;: 400000, &#x27;num_env_steps_trained&#x27;: 400000, &#x27;num_agent_steps_sampled&#x27;: 400000, &#x27;num_agent_steps_trained&#x27;: 400000}</td><td>{}              </td><td>2022-11-21_19-57-27</td><td>True  </td><td style=\"text-align: right;\">               164</td><td>{}             </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  164</td><td style=\"text-align: right;\">                 124</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            2775</td><td>c2cd0e835f994fe19e28f35d1bdaa2d1</td><td>jim-MS-7998</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 27.49564595256121, &#x27;cur_kl_coeff&#x27;: 1.4551915228366855e-12, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 8.745076330759192, &#x27;policy_loss&#x27;: 0.0016528649586102656, &#x27;vf_loss&#x27;: 8.743423469604984, &#x27;vf_explained_var&#x27;: -0.7099209619465695, &#x27;kl&#x27;: 0.008644306757214934, &#x27;entropy&#x27;: 0.42829842468102775, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 400000, &#x27;num_env_steps_trained&#x27;: 400000, &#x27;num_agent_steps_sampled&#x27;: 400000, &#x27;num_agent_steps_trained&#x27;: 400000}</td><td style=\"text-align: right;\">                       100</td><td>192.168.0.18</td><td style=\"text-align: right;\">                   400000</td><td style=\"text-align: right;\">                   400000</td><td style=\"text-align: right;\">                 400000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                 400000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 38.088888888888896, &#x27;ram_util_percent&#x27;: 62.0}</td><td style=\"text-align: right;\">10319</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.25664393837396066, &#x27;mean_inference_ms&#x27;: 0.8829934073394802, &#x27;mean_action_processing_ms&#x27;: 0.05694164016467438, &#x27;mean_env_wait_ms&#x27;: 0.06483274197641808, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 200.0, &#x27;episode_reward_min&#x27;: 124.0, &#x27;episode_reward_mean&#x27;: 164.0, &#x27;episode_len_mean&#x27;: 164.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 23, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [134.0, 142.0, 135.0, 167.0, 157.0, 155.0, 148.0, 163.0, 149.0, 177.0, 124.0, 176.0, 170.0, 180.0, 158.0, 165.0, 131.0, 168.0, 173.0, 153.0, 174.0, 155.0, 168.0, 152.0, 150.0, 171.0, 153.0, 162.0, 138.0, 151.0, 162.0, 182.0, 199.0, 153.0, 177.0, 162.0, 200.0, 161.0, 150.0, 155.0, 153.0, 146.0, 182.0, 177.0, 188.0, 198.0, 164.0, 151.0, 163.0, 143.0, 161.0, 190.0, 171.0, 165.0, 194.0, 200.0, 138.0, 169.0, 147.0, 147.0, 184.0, 185.0, 178.0, 142.0, 167.0, 175.0, 174.0, 156.0, 176.0, 160.0, 146.0, 131.0, 152.0, 197.0, 167.0, 159.0, 150.0, 164.0, 149.0, 173.0, 189.0, 165.0, 132.0, 128.0, 200.0, 172.0, 159.0, 149.0, 194.0, 157.0, 140.0, 200.0, 149.0, 185.0, 147.0, 169.0, 197.0, 184.0, 162.0, 190.0], &#x27;episode_lengths&#x27;: [134, 142, 135, 167, 157, 155, 148, 163, 149, 177, 124, 176, 170, 180, 158, 165, 131, 168, 173, 153, 174, 155, 168, 152, 150, 171, 153, 162, 138, 151, 162, 182, 199, 153, 177, 162, 200, 161, 150, 155, 153, 146, 182, 177, 188, 198, 164, 151, 163, 143, 161, 190, 171, 165, 194, 200, 138, 169, 147, 147, 184, 185, 178, 142, 167, 175, 174, 156, 176, 160, 146, 131, 152, 197, 167, 159, 150, 164, 149, 173, 189, 165, 132, 128, 200, 172, 159, 149, 194, 157, 140, 200, 149, 185, 147, 169, 197, 184, 162, 190]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.25664393837396066, &#x27;mean_inference_ms&#x27;: 0.8829934073394802, &#x27;mean_action_processing_ms&#x27;: 0.05694164016467438, &#x27;mean_env_wait_ms&#x27;: 0.06483274197641808, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             667.143</td><td style=\"text-align: right;\">           6.26948</td><td style=\"text-align: right;\">       667.143</td><td>{&#x27;training_iteration_time_ms&#x27;: 6274.882, &#x27;load_time_ms&#x27;: 0.256, &#x27;load_throughput&#x27;: 15608164.48, &#x27;learn_time_ms&#x27;: 3881.03, &#x27;learn_throughput&#x27;: 1030.654, &#x27;synch_weights_time_ms&#x27;: 1.145}</td><td style=\"text-align: right;\"> 1669078647</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">           400000</td><td style=\"text-align: right;\">                 100</td><td>0f402_00000</td><td style=\"text-align: right;\">      3.68059</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 19:57:27,999\tINFO tune.py:777 -- Total run time: 676.82 seconds (676.21 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f21392e7ac0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "#     # Set up a separate evaluation worker set for the\n",
    "#     # `algo.evaluate()` call after training (see below).\n",
    "#     \"evaluation_num_workers\": 1,\n",
    "#     # Only for evaluation runs, render the env.\n",
    "#     \"evaluation_config\": {\n",
    "#         \"render_env\": True,\n",
    "#     },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Train via Ray Tune.\n",
    "# Note that Ray Tune does not yet support AlgorithmConfig objects, hence\n",
    "# we need to convert back to old-style config dicts.\n",
    "tune.run(\"PPO\", \n",
    "        config=config, \n",
    "        stop={\"training_iteration\": 100},\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir='ray210_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ada2136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-21 20:08:45</td></tr>\n",
       "<tr><td>Running for: </td><td>00:11:17.03        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.7/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/2.42 GiB heap, 0.0/1.21 GiB objects (0.0/1.0 accelerator_type:GTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_FrozenLake-v1_a2b48_00000</td><td>TERMINATED</td><td>192.168.0.18:11123</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         667.691</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">0.111546</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11123)\u001b[0m 2022-11-21 19:57:31,642\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=11123)\u001b[0m 2022-11-21 19:57:31,644\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11159)\u001b[0m 2022-11-21 19:57:34,232\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=11123)\u001b[0m 2022-11-21 19:57:34,380\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname   </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_FrozenLake-v1_a2b48_00000</td><td style=\"text-align: right;\">                 400000</td><td>{&#x27;num_env_steps_sampled&#x27;: 400000, &#x27;num_env_steps_trained&#x27;: 400000, &#x27;num_agent_steps_sampled&#x27;: 400000, &#x27;num_agent_steps_trained&#x27;: 400000}</td><td>{}              </td><td>2022-11-21_20-08-45</td><td>True  </td><td style=\"text-align: right;\">           7.84149</td><td>{}             </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">             0.111546</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 511</td><td style=\"text-align: right;\">           56320</td><td>d114bcadc03b4aae952fb6b573149205</td><td>jim-MS-7998</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 0.37471067850948664, &#x27;cur_kl_coeff&#x27;: 8.077935669463162e-29, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 0.10052910307522422, &#x27;policy_loss&#x27;: -0.00356466507398954, &#x27;vf_loss&#x27;: 0.10409376773863069, &#x27;vf_explained_var&#x27;: 0.1338516867288979, &#x27;kl&#x27;: 0.00035450372845896594, &#x27;entropy&#x27;: 0.05088595755758785, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 400000, &#x27;num_env_steps_trained&#x27;: 400000, &#x27;num_agent_steps_sampled&#x27;: 400000, &#x27;num_agent_steps_trained&#x27;: 400000}</td><td style=\"text-align: right;\">                       100</td><td>192.168.0.18</td><td style=\"text-align: right;\">                   400000</td><td style=\"text-align: right;\">                   400000</td><td style=\"text-align: right;\">                 400000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                 400000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 39.22222222222222, &#x27;ram_util_percent&#x27;: 62.20000000000001}</td><td style=\"text-align: right;\">11123</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.33718412694598293, &#x27;mean_inference_ms&#x27;: 0.834718132169564, &#x27;mean_action_processing_ms&#x27;: 0.05388721483819092, &#x27;mean_env_wait_ms&#x27;: 0.06541544005311958, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 1.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 0.11154598825831702, &#x27;episode_len_mean&#x27;: 7.841487279843444, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 511, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;episode_lengths&#x27;: [7, 15, 4, 4, 9, 5, 4, 3, 3, 5, 2, 6, 12, 7, 9, 16, 6, 2, 9, 11, 11, 2, 3, 9, 3, 5, 5, 11, 10, 2, 17, 2, 13, 6, 10, 10, 2, 21, 3, 7, 2, 4, 13, 2, 9, 6, 11, 32, 7, 14, 4, 3, 7, 5, 2, 3, 6, 7, 3, 4, 11, 6, 4, 2, 10, 5, 10, 5, 3, 2, 10, 10, 8, 10, 6, 4, 15, 3, 17, 5, 6, 9, 12, 10, 11, 2, 4, 9, 8, 5, 9, 8, 4, 6, 3, 3, 3, 2, 5, 2, 15, 2, 11, 9, 10, 3, 6, 2, 3, 15, 2, 6, 4, 9, 7, 5, 4, 6, 24, 13, 8, 4, 10, 14, 2, 21, 7, 6, 8, 29, 24, 4, 24, 4, 7, 4, 4, 15, 2, 2, 5, 3, 13, 3, 6, 14, 42, 10, 4, 5, 5, 3, 21, 3, 17, 12, 5, 2, 12, 2, 8, 16, 7, 4, 7, 18, 7, 5, 2, 5, 14, 23, 2, 7, 2, 9, 14, 7, 6, 5, 15, 7, 6, 7, 10, 22, 10, 8, 4, 2, 2, 20, 7, 5, 9, 7, 11, 18, 13, 3, 24, 3, 4, 4, 14, 20, 3, 11, 9, 2, 7, 4, 4, 5, 7, 5, 4, 2, 8, 2, 5, 6, 2, 4, 9, 6, 3, 8, 3, 5, 2, 9, 14, 12, 11, 10, 2, 6, 14, 8, 12, 8, 20, 6, 13, 5, 12, 9, 9, 12, 9, 3, 6, 25, 46, 7, 11, 11, 7, 4, 7, 8, 13, 7, 3, 9, 6, 3, 8, 6, 11, 3, 3, 8, 5, 11, 20, 15, 10, 14, 9, 8, 4, 6, 2, 5, 27, 11, 2, 8, 3, 9, 19, 5, 4, 6, 5, 3, 2, 13, 6, 4, 3, 5, 7, 2, 5, 2, 6, 9, 9, 10, 4, 8, 8, 5, 9, 14, 6, 10, 8, 13, 14, 4, 8, 11, 5, 6, 8, 4, 6, 13, 14, 6, 5, 11, 15, 5, 10, 12, 25, 11, 2, 18, 8, 8, 10, 2, 14, 8, 9, 5, 2, 7, 2, 8, 7, 24, 16, 3, 4, 7, 3, 3, 14, 5, 4, 4, 7, 14, 6, 15, 14, 4, 13, 13, 6, 2, 5, 15, 4, 9, 8, 3, 4, 6, 10, 2, 4, 9, 4, 5, 7, 2, 7, 4, 9, 5, 6, 4, 12, 35, 2, 6, 4, 10, 8, 3, 2, 5, 6, 9, 18, 14, 7, 14, 6, 2, 8, 3, 10, 4, 9, 3, 6, 9, 2, 10, 9, 5, 6, 11, 7, 2, 12, 5, 5, 12, 5, 2, 11, 10, 12, 13, 4, 4, 5, 4, 7, 2, 4, 2, 2, 9, 6, 13, 14, 8, 7, 3, 3, 10, 2, 3, 16, 8, 3, 6, 2, 2, 12, 4, 12, 13, 9, 9, 12, 5, 2, 5, 4, 2, 8, 4, 13, 2, 2, 13, 2, 3, 19, 3, 4, 5, 5, 4, 12, 10, 4, 8, 11, 21, 6, 4, 7, 13, 4, 3, 6, 7, 8]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.33718412694598293, &#x27;mean_inference_ms&#x27;: 0.834718132169564, &#x27;mean_action_processing_ms&#x27;: 0.05388721483819092, &#x27;mean_env_wait_ms&#x27;: 0.06541544005311958, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             667.691</td><td style=\"text-align: right;\">            6.7206</td><td style=\"text-align: right;\">       667.691</td><td>{&#x27;training_iteration_time_ms&#x27;: 6650.37, &#x27;load_time_ms&#x27;: 0.423, &#x27;load_throughput&#x27;: 9451952.676, &#x27;learn_time_ms&#x27;: 4019.109, &#x27;learn_throughput&#x27;: 995.245, &#x27;synch_weights_time_ms&#x27;: 1.127}</td><td style=\"text-align: right;\"> 1669079325</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">           400000</td><td style=\"text-align: right;\">                 100</td><td>a2b48_00000</td><td style=\"text-align: right;\">      2.74263</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:08:45,871\tINFO tune.py:777 -- Total run time: 677.81 seconds (677.01 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f2138d76bb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"FrozenLake-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "#     # Set up a separate evaluation worker set for the\n",
    "#     # `algo.evaluate()` call after training (see below).\n",
    "#     \"evaluation_num_workers\": 1,\n",
    "#     # Only for evaluation runs, render the env.\n",
    "#     \"evaluation_config\": {\n",
    "#         \"render_env\": True,\n",
    "#     },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Train via Ray Tune.\n",
    "# Note that Ray Tune does not yet support AlgorithmConfig objects, hence\n",
    "# we need to convert back to old-style config dicts.\n",
    "tune.run(\"PPO\", \n",
    "        config=config, \n",
    "        stop={\"training_iteration\": 100},\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir='ray210_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bd9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba630d6",
   "metadata": {},
   "source": [
    "### Test Loading and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce155ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:39:52,231\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-11-22 19:39:52,234\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20618)\u001b[0m /home/jim/anaconda3/envs/rlease2022/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20618)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20618)\u001b[0m 2022-11-22 19:39:55,337\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2022-11-22 19:39:55,499\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not recover from checkpoint as it does not exist on local disk and was not available on cloud storage or another Ray node. Got checkpoint path: saved_agents/ray210_results/PPO/PPO_CartPole-v0_3_iters/checkpoint_000003/algorithm_state.pkl and IP None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m restore_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m         }\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer(config\u001b[38;5;241m=\u001b[39mrestore_config, env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestore_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rlease2022/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:729\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path, checkpoint_node_ip, fallback_to_latest)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore(checkpoint_path, fallback_to_latest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# Else, raise\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not recover from checkpoint as it does not exist on local \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk and was not available on cloud storage or another Ray node. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and IP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_node_ip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    735\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m TrainableUtil\u001b[38;5;241m.\u001b[39mfind_checkpoint_dir(checkpoint_path)\n\u001b[1;32m    736\u001b[0m metadata \u001b[38;5;241m=\u001b[39m TrainableUtil\u001b[38;5;241m.\u001b[39mload_metadata(checkpoint_dir)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not recover from checkpoint as it does not exist on local disk and was not available on cloud storage or another Ray node. Got checkpoint path: saved_agents/ray210_results/PPO/PPO_CartPole-v0_3_iters/checkpoint_000003/algorithm_state.pkl and IP None"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "restore_path = 'saved_agents/ray210_results/PPO/PPO_CartPole-v0_3_iters/checkpoint_000003/algorithm_state.pkl'\n",
    "\n",
    "restore_config = {\n",
    "            \"framework\": \"torch\",\n",
    "            \"num_workers\": 1,\n",
    "        }\n",
    "\n",
    "trainer = PPOTrainer(config=restore_config, env=\"CartPole-v0\")\n",
    "trainer.restore(restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61e8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlease2022",
   "language": "python",
   "name": "rlease2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
