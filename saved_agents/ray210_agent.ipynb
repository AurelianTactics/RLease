{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07108da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-21 19:49:51</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:39.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.7/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/2.42 GiB heap, 0.0/1.21 GiB objects (0.0/1.0 accelerator_type:GTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_0f402_00000</td><td>RUNNING </td><td>192.168.0.18:10319</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         207.719</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  184.86</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  34</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:14,024\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:14,025\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10362)\u001b[0m /home/jim/anaconda3/envs/rlease2022/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10362)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m /home/jim/anaconda3/envs/rlease2022/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=10361)\u001b[0m 2022-11-21 19:46:17,613\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=10319)\u001b[0m 2022-11-21 19:46:17,699\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname   </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                       </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                  </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_0f402_00000</td><td style=\"text-align: right;\">                 124000</td><td>{&#x27;num_env_steps_sampled&#x27;: 124000, &#x27;num_env_steps_trained&#x27;: 124000, &#x27;num_agent_steps_sampled&#x27;: 124000, &#x27;num_agent_steps_trained&#x27;: 124000}</td><td>{}              </td><td>2022-11-21_19-49-52</td><td>False </td><td style=\"text-align: right;\">            179.01</td><td>{}             </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               179.01</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">            1033</td><td>c2cd0e835f994fe19e28f35d1bdaa2d1</td><td>jim-MS-7998</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 4.942583326739009, &#x27;cur_kl_coeff&#x27;: 1.2207031250000002e-05, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 9.639370515782346, &#x27;policy_loss&#x27;: 0.002644715098644136, &#x27;vf_loss&#x27;: 9.636725800011748, &#x27;vf_explained_var&#x27;: -0.7383503538306041, &#x27;kl&#x27;: 0.00336119737431245, &#x27;entropy&#x27;: 0.5785957704308212, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0}}, &#x27;num_env_steps_sampled&#x27;: 124000, &#x27;num_env_steps_trained&#x27;: 124000, &#x27;num_agent_steps_sampled&#x27;: 124000, &#x27;num_agent_steps_trained&#x27;: 124000}</td><td style=\"text-align: right;\">                        31</td><td>192.168.0.18</td><td style=\"text-align: right;\">                   124000</td><td style=\"text-align: right;\">                   124000</td><td style=\"text-align: right;\">                 124000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                 124000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 44.333333333333336, &#x27;ram_util_percent&#x27;: 62.29999999999999}</td><td style=\"text-align: right;\">10319</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.27098740473123156, &#x27;mean_inference_ms&#x27;: 0.9219595559329484, &#x27;mean_action_processing_ms&#x27;: 0.059155949391140314, &#x27;mean_env_wait_ms&#x27;: 0.06760820896984958, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 200.0, &#x27;episode_reward_min&#x27;: 27.0, &#x27;episode_reward_mean&#x27;: 179.01, &#x27;episode_len_mean&#x27;: 179.01, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 26, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [182.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 193.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 189.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 123.0, 200.0, 169.0, 199.0, 177.0, 189.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 180.0, 119.0, 200.0, 200.0, 151.0, 151.0, 165.0, 176.0, 124.0, 200.0, 161.0, 177.0, 169.0, 200.0, 121.0, 186.0, 180.0, 193.0, 200.0, 135.0, 185.0, 148.0, 186.0, 149.0, 167.0, 185.0, 192.0, 184.0, 200.0, 149.0, 124.0, 175.0, 157.0, 27.0, 164.0, 179.0, 185.0, 146.0, 141.0, 174.0, 200.0, 190.0, 170.0, 137.0, 138.0, 98.0, 125.0, 126.0, 133.0, 166.0], &#x27;episode_lengths&#x27;: [182, 200, 200, 200, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 189, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 123, 200, 169, 199, 177, 189, 200, 200, 192, 200, 200, 200, 200, 180, 119, 200, 200, 151, 151, 165, 176, 124, 200, 161, 177, 169, 200, 121, 186, 180, 193, 200, 135, 185, 148, 186, 149, 167, 185, 192, 184, 200, 149, 124, 175, 157, 27, 164, 179, 185, 146, 141, 174, 200, 190, 170, 137, 138, 98, 125, 126, 133, 166]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.27098740473123156, &#x27;mean_inference_ms&#x27;: 0.9219595559329484, &#x27;mean_action_processing_ms&#x27;: 0.059155949391140314, &#x27;mean_env_wait_ms&#x27;: 0.06760820896984958, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             214.435</td><td style=\"text-align: right;\">            6.7163</td><td style=\"text-align: right;\">       214.435</td><td>{&#x27;training_iteration_time_ms&#x27;: 6794.367, &#x27;load_time_ms&#x27;: 0.314, &#x27;load_throughput&#x27;: 12729298.938, &#x27;learn_time_ms&#x27;: 4156.869, &#x27;learn_throughput&#x27;: 962.263, &#x27;synch_weights_time_ms&#x27;: 1.133}</td><td style=\"text-align: right;\"> 1669078192</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">           124000</td><td style=\"text-align: right;\">                  31</td><td>0f402_00000</td><td style=\"text-align: right;\">      3.68059</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "#     # Set up a separate evaluation worker set for the\n",
    "#     # `algo.evaluate()` call after training (see below).\n",
    "#     \"evaluation_num_workers\": 1,\n",
    "#     # Only for evaluation runs, render the env.\n",
    "#     \"evaluation_config\": {\n",
    "#         \"render_env\": True,\n",
    "#     },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Train via Ray Tune.\n",
    "# Note that Ray Tune does not yet support AlgorithmConfig objects, hence\n",
    "# we need to convert back to old-style config dicts.\n",
    "tune.run(\"PPO\", \n",
    "        config=config, \n",
    "        stop={\"training_iteration\": 100},\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir='ray210_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34638641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"FrozenLake-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "#     # Set up a separate evaluation worker set for the\n",
    "#     # `algo.evaluate()` call after training (see below).\n",
    "#     \"evaluation_num_workers\": 1,\n",
    "#     # Only for evaluation runs, render the env.\n",
    "#     \"evaluation_config\": {\n",
    "#         \"render_env\": True,\n",
    "#     },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Train via Ray Tune.\n",
    "# Note that Ray Tune does not yet support AlgorithmConfig objects, hence\n",
    "# we need to convert back to old-style config dicts.\n",
    "tune.run(\"PPO\", \n",
    "        config=config, \n",
    "        stop={\"training_iteration\": 100},\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir='ray210_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ffd0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlease2022",
   "language": "python",
   "name": "rlease2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
